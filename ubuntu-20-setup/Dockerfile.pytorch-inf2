# Example pytorch neuron container
# To build:
#    docker build . -f Dockerfile.pt -t neuron-container:pytorch
# To run on EC2 Inf1 instances with AWS DLAMI:
#    docker run -it --net=host --device=/dev/neuron0 neuron-container:pytorch

FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference-neuronx:1.13.1-neuronx-py310-sdk2.14.1-ubuntu20.04

# # Configure tensorboard for trn1
# RUN python3 -m pip install --upgrade tensorboard
# RUN python3 -m pip config set global.extra-index-url "https://pip.repos.neuron.amazonaws.com"
# RUN python3 -m pip install tensorboard-plugin-neuronx

# # ONLY for inf1
# # create a directory where profile data will be dumped and set the NEURON_PROFILE environment variable.
# RUN mkdir -p $HOME/profile
# ENV NEURON_PROFILE=$HOME/profile
# # 4.2. Ensure Neuron Tools are executable by setting the PATH environment variable.
# ENV PATH=/opt/aws/neuron/bin:$PATH



# RUN curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj -C /usr/local/bin --strip-components 1
# # Set PATH so micromamba is found
# ENV PATH="/usr/local/bin:${PATH}"
# COPY conda.yaml conda.yaml
# COPY requirements.txt requirements.txt
# # Use micromamba to install packages from the conda.yml file into the base environment
# RUN micromamba install -y -n base -f conda.yaml && \
# micromamba clean --all --yes
# ENV PATH="/root/micromamba/bin:${PATH}"
# RUN mkdir -p /opt/ml
WORKDIR /opt/app/traiunium